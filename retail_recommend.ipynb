{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Engine for E-Commerce Sales\n",
    "\n",
    "This notebook gives an overview of techniques and services offer by SageMaker to build and deploy a personalized recommendation engine.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset for this demo comes from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail). It contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. The following attributes are included in our dataset:\n",
    "+ InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
    "+ StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "+ Description: Product (item) name. Nominal.\n",
    "+ Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "+ InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
    "+ UnitPrice: Unit price. Numeric, Product price per unit in sterling.\n",
    "+ CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
    "+ Country: Country name. Nominal, the name of the country where each customer resides. \n",
    "\n",
    "Citation: Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197â€“208, 2012 (Published online before print: 27 August 2012. doi: 10.1057/dbm.2012.17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation\n",
    "----\n",
    "The first of the notebook will focus on preparing the data for training.\n",
    "\n",
    "### Solution Architecture\n",
    "![Architecture](./images/retail_rec_dataprep.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (2.110.0)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (4.12.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (3.20.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.21.6)\n",
      "Requirement already satisfied: attrs<22,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (21.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.3.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.1)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.24.62)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.62 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.27.62)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: pox>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.1)\n",
      "Requirement already satisfied: multiprocess>=0.70.13 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.13)\n",
      "Requirement already satisfied: dill>=0.3.5.1 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.5.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.5 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (1.7.6.5)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.62->boto3<2.0,>=1.20.21->sagemaker) (1.26.12)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "import boto3\n",
    "\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack, save_npz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert sagemaker.__version__ >= \"2.21.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.110.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using bucketsagemaker-ap-northeast-2-337402169710 in region ap-northeast-2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "region = boto3.Session().region_name\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(f\"using bucket{bucket} in region {region} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(541909, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>2.55</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceNo StockCode                          Description  Quantity  \\\n",
       "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
       "1    536365     71053                  WHITE METAL LANTERN         6   \n",
       "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
       "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
       "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
       "\n",
       "           InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "0  2010-12-01 08:26:00       2.55     17850.0  United Kingdom  \n",
       "1  2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
       "2  2010-12-01 08:26:00       2.75     17850.0  United Kingdom  \n",
       "3  2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
       "4  2010-12-01 08:26:00       3.39     17850.0  United Kingdom  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/Online Retail.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "First, we check for any null (i.e. missing) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvoiceNo           0\n",
       "StockCode           0\n",
       "Description      1454\n",
       "Quantity            0\n",
       "InvoiceDate         0\n",
       "UnitPrice           0\n",
       "CustomerID     135080\n",
       "Country             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop any records with a missing CustomerID. If we do not know who the customer is, then it is not helpful to us when we make recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406829, 8)\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=[\"CustomerID\"], inplace=True)\n",
    "df[\"Description\"] = df[\"Description\"].apply(lambda x: x.strip())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.distplot(df[\"Quantity\"], kde=True)\n",
    "plt.title(\"Distribution of Quantity\")\n",
    "plt.xlabel(\"Quantity\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our quantities are realteively small (positive) numbers, but there are also some negative quantities as well as extreme outliers (both postiive and negative outliers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.distplot(df[\"UnitPrice\"], kde=True)\n",
    "plt.title(\"Distribution of Unit Prices\")\n",
    "plt.xlabel(\"Price\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no negative prices, which is good, but we can see some extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quantity</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>406829.000000</td>\n",
       "      <td>406829.000000</td>\n",
       "      <td>406829.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.061303</td>\n",
       "      <td>3.460471</td>\n",
       "      <td>15287.690570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>248.693370</td>\n",
       "      <td>69.315162</td>\n",
       "      <td>1713.600303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-80995.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12346.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>13953.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>15152.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>16791.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80995.000000</td>\n",
       "      <td>38970.000000</td>\n",
       "      <td>18287.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Quantity      UnitPrice     CustomerID\n",
       "count  406829.000000  406829.000000  406829.000000\n",
       "mean       12.061303       3.460471   15287.690570\n",
       "std       248.693370      69.315162    1713.600303\n",
       "min    -80995.000000       0.000000   12346.000000\n",
       "25%         2.000000       1.250000   13953.000000\n",
       "50%         5.000000       1.950000   15152.000000\n",
       "75%        12.000000       3.750000   16791.000000\n",
       "max     80995.000000   38970.000000   18287.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274399, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.groupby([\"StockCode\", \"Description\", \"CustomerID\", \"Country\", \"UnitPrice\"])[\n",
    "    \"Quantity\"\n",
    "].sum()\n",
    "df = df.loc[df > 0].reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(dataframe):\n",
    "    enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    onehot_cols = [\"StockCode\", \"CustomerID\", \"Country\"]\n",
    "    ohe_output = enc.fit_transform(dataframe[onehot_cols])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=2)\n",
    "    unique_descriptions = dataframe[\"Description\"].unique()\n",
    "    vectorizer.fit(unique_descriptions)\n",
    "    tfidf_output = vectorizer.transform(dataframe[\"Description\"])\n",
    "\n",
    "    row = range(len(dataframe))\n",
    "    col = [0] * len(dataframe)\n",
    "    unit_price = csr_matrix((dataframe[\"UnitPrice\"].values, (row, col)), dtype=\"float32\")\n",
    "\n",
    "    X = hstack([ohe_output, tfidf_output, unit_price], format=\"csr\", dtype=\"float32\")\n",
    "\n",
    "    y = dataframe[\"Quantity\"].values.astype(\"float32\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = loadDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991284988048746"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display sparsity\n",
    "total_cells = X.shape[0] * X.shape[1]\n",
    "(total_cells - X.nnz) / total_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is over 99.9% sparse. Because of this high sparsity, the sparse matrix data type allows us to represent our data using only a small fraction of the memory that a dense matrix would require."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data For Modeling\n",
    "\n",
    "+ Split the data into training and testing sets\n",
    "+ Write the data to protobuf recordIO format for Pipe mode. [Read more](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html) about protobuf recordIO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((219519, 9284), (54880, 9284), (219519,), (54880,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save numpy arrays to local storage in /data folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/online_retail_preprocessed.csv\", index=False)\n",
    "save_npz(\"data/X_train.npz\", X_train)\n",
    "save_npz(\"data/X_test.npz\", X_test)\n",
    "np.savez(\"data/y_train.npz\", y_train)\n",
    "np.savez(\"data/y_test.npz\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"personalization\"\n",
    "\n",
    "train_key = \"train.protobuf\"\n",
    "train_prefix = f\"{prefix}/train\"\n",
    "\n",
    "test_key = \"test.protobuf\"\n",
    "test_prefix = f\"{prefix}/test\"\n",
    "\n",
    "output_prefix = f\"s3://{bucket}/{prefix}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-northeast-2-337402169710/personalization/train/train.protobuf\n",
      "s3://sagemaker-ap-northeast-2-337402169710/personalization/test/test.protobuf\n",
      "Output: s3://sagemaker-ap-northeast-2-337402169710/personalization/output\n"
     ]
    }
   ],
   "source": [
    "def writeDatasetToProtobuf(X, y, bucket, prefix, key):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, y)\n",
    "    buf.seek(0)\n",
    "    obj = \"{}/{}\".format(prefix, key)\n",
    "    boto3.resource(\"s3\").Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return \"s3://{}/{}\".format(bucket, obj)\n",
    "\n",
    "\n",
    "train_data_location = writeDatasetToProtobuf(X_train, y_train, bucket, train_prefix, train_key)\n",
    "test_data_location = writeDatasetToProtobuf(X_test, y_test, bucket, test_prefix, test_key)\n",
    "\n",
    "print(train_data_location)\n",
    "print(test_data_location)\n",
    "print(\"Output: {}\".format(output_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Train, Tune, and Deploy Model\n",
    "----\n",
    "This second part will focus on training, tuning, and deploying a model trained on the data prepared in part 1.\n",
    "\n",
    "### Solution Architecture\n",
    "![Architecture](./images/retail_rec_train_reg_deploy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.lineage import context, artifact, association, action\n",
    "import boto3\n",
    "\n",
    "from model_package_src.inference_specification import InferenceSpecification\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from scipy.sparse import csr_matrix, hstack, load_npz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix = \"personalization\"\n",
    "\n",
    "output_prefix = f\"s3://{bucket}/{prefix}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data For Modeling\n",
    "\n",
    "+ Split the data into training and testing sets\n",
    "+ Write the data to protobuf recordIO format for Pipe mode. [Read more](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html) about protobuf recordIO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((219519, 9284), (54880, 9284), (219519,), (54880,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load array\n",
    "X_train = load_npz(\"./data/X_train.npz\")\n",
    "X_test = load_npz(\"./data/X_test.npz\")\n",
    "y_train_npzfile = np.load(\"./data/y_train.npz\")\n",
    "y_test_npzfile = np.load(\"./data/y_test.npz\")\n",
    "y_train = y_train_npzfile.f.arr_0\n",
    "y_test = y_test_npzfile.f.arr_0\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(\"factorization-machines\", region=boto_session.region_name)\n",
    "\n",
    "fm = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    output_path=output_prefix,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "    feature_dim=input_dims,\n",
    "    predictor_type=\"regressor\",\n",
    "    mini_batch_size=1000,\n",
    "    num_factors=64,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-29 14:34:33 Starting - Starting the training job...\n",
      "2022-09-29 14:34:57 Starting - Preparing the instances for trainingProfilerReport-1664462073: InProgress\n",
      "......\n",
      "2022-09-29 14:35:57 Downloading - Downloading input data...\n",
      "2022-09-29 14:36:17 Training - Downloading the training image............\n",
      "2022-09-29 14:38:26 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:31 INFO 139757888653120] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:31 INFO 139757888653120] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'epochs': '20', 'feature_dim': '9284', 'mini_batch_size': '1000', 'num_factors': '64', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:31 INFO 139757888653120] Final configuration: {'epochs': '20', 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '9284', 'num_factors': '64', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:31 WARNING 139757888653120] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:31 INFO 139757888653120] Using default worker.\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:31 INFO 139757888653120] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:38:31.440] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:38:31.451] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 17, \"num_examples\": 1, \"num_bytes\": 99840}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:31 INFO 139757888653120] nvidia-smi: took 0.033 seconds to run.\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:31 INFO 139757888653120] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:31 INFO 139757888653120] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:31 INFO 139757888653120] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:31 INFO 139757888653120] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462311.4335377, \"EndTime\": 1664462311.4874182, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 40.008544921875, \"count\": 1, \"min\": 40.008544921875, \"max\": 40.008544921875}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462311.4875348, \"EndTime\": 1664462311.487608, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[14:38:31] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.16.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:306: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use kv.row_sparse_pull() or module.prepare() with row_ids.\u001b[0m\n",
      "\u001b[34m[14:38:31] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.16.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:306: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use kv.row_sparse_pull() or module.prepare() with row_ids.\u001b[0m\n",
      "\u001b[34m[14:38:31] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.16.0/AL2_x86_64/generic-flavor/src/src/operator/././../common/utils.h:450: Optimizer with lazy_update = True detected. Be aware that lazy update with row_sparse gradient is different from standard update, and may lead to different empirical results. See https://mxnet.incubator.apache.org/api/python/optimization/optimization.html for more details.\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:31 INFO 139757888653120] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=72.23537568255598\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:31 INFO 139757888653120] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=5217.9495\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:31 INFO 139757888653120] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=17.686853515625\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:38:34.096] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 2564, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:34 INFO 139757888653120] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=91.21372962998149\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:34 INFO 139757888653120] #quality_metric: host=algo-1, epoch=0, train mse <loss>=8319.944473011363\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:34 INFO 139757888653120] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=16.155873508522728\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462311.4874878, \"EndTime\": 1664462314.0971687, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"update.time\": {\"sum\": 2609.2822551727295, \"count\": 1, \"min\": 2609.2822551727295, \"max\": 2609.2822551727295}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:34 INFO 139757888653120] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462311.4878597, \"EndTime\": 1664462314.0973938, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 220519.0, \"count\": 1, \"min\": 220519, \"max\": 220519}, \"Total Batches Seen\": {\"sum\": 221.0, \"count\": 1, \"min\": 221, \"max\": 221}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:34 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=84118.52592872766 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:34 INFO 139757888653120] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=70.05814727781488\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:34 INFO 139757888653120] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=4908.144\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:34 INFO 139757888653120] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=17.394908203125\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:38:36.397] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 2295, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:36 INFO 139757888653120] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=90.60649097038197\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:36 INFO 139757888653120] #quality_metric: host=algo-1, epoch=1, train mse <loss>=8209.53620596591\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:36 INFO 139757888653120] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=18.40966073774858\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462314.097242, \"EndTime\": 1664462316.3975809, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2299.039602279663, \"count\": 1, \"min\": 2299.039602279663, \"max\": 2299.039602279663}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:36 INFO 139757888653120] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462314.0985174, \"EndTime\": 1664462316.397745, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 440038.0, \"count\": 1, \"min\": 440038, \"max\": 440038}, \"Total Batches Seen\": {\"sum\": 441.0, \"count\": 1, \"min\": 441, \"max\": 441}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:36 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=95471.844666766 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:36 INFO 139757888653120] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=69.88720197575519\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:36 INFO 139757888653120] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=4884.221\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:36 INFO 139757888653120] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=18.548525390625\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:38:38.653] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 2252, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:38 INFO 139757888653120] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=90.47292685050996\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:38 INFO 139757888653120] #quality_metric: host=algo-1, epoch=2, train mse <loss>=8185.350492897727\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:38 INFO 139757888653120] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=18.57761301047585\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462316.397632, \"EndTime\": 1664462318.6544433, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2255.7356357574463, \"count\": 1, \"min\": 2255.7356357574463, \"max\": 2255.7356357574463}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:38 INFO 139757888653120] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462316.3986866, \"EndTime\": 1664462318.65458, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 659557.0, \"count\": 1, \"min\": 659557, \"max\": 659557}, \"Total Batches Seen\": {\"sum\": 661.0, \"count\": 1, \"min\": 661, \"max\": 661}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:38 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=97305.71234826786 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:38 INFO 139757888653120] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=69.72644763646001\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:38 INFO 139757888653120] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=4861.7775\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:38 INFO 139757888653120] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=18.329232421875\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:38:40.901] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 2242, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:40 INFO 139757888653120] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=90.33221964170772\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:40 INFO 139757888653120] #quality_metric: host=algo-1, epoch=3, train mse <loss>=8159.909905397727\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:40 INFO 139757888653120] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=18.271448410866476\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462318.6544952, \"EndTime\": 1664462320.9017105, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2245.9821701049805, \"count\": 1, \"min\": 2245.9821701049805, \"max\": 2245.9821701049805}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:40 INFO 139757888653120] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462318.6557033, \"EndTime\": 1664462320.9019156, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 879076.0, \"count\": 1, \"min\": 879076, \"max\": 879076}, \"Total Batches Seen\": {\"sum\": 881.0, \"count\": 1, \"min\": 881, \"max\": 881}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:40 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=97723.00259068595 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:40 INFO 139757888653120] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=69.57357256890003\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:40 INFO 139757888653120] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=4840.482\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:40 INFO 139757888653120] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=18.078337890625\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:38:43.113] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 2206, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:43 INFO 139757888653120] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=90.20135544911703\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:43 INFO 139757888653120] #quality_metric: host=algo-1, epoch=4, train mse <loss>=8136.284524857954\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:43 INFO 139757888653120] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=18.019270476740058\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462320.901772, \"EndTime\": 1664462323.114229, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2211.247682571411, \"count\": 1, \"min\": 2211.247682571411, \"max\": 2211.247682571411}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:43 INFO 139757888653120] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462320.902957, \"EndTime\": 1664462323.114437, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1098595.0, \"count\": 1, \"min\": 1098595, \"max\": 1098595}, \"Total Batches Seen\": {\"sum\": 1101.0, \"count\": 1, \"min\": 1101, \"max\": 1101}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:43 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=99258.57815158555 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:43 INFO 139757888653120] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=69.43905601316884\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:43 INFO 139757888653120] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=4821.7825\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:43 INFO 139757888653120] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=17.8794375\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:38:45.367] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 2247, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:45 INFO 139757888653120] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=90.08462691645083\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:45 INFO 139757888653120] #quality_metric: host=algo-1, epoch=5, train mse <loss>=8115.2400066761365\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:45 INFO 139757888653120] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=17.84477959428267\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462323.1142886, \"EndTime\": 1664462325.3680303, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2252.3226737976074, \"count\": 1, \"min\": 2252.3226737976074, \"max\": 2252.3226737976074}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:45 INFO 139757888653120] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462323.1156833, \"EndTime\": 1664462325.3682044, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1318114.0, \"count\": 1, \"min\": 1318114, \"max\": 1318114}, \"Total Batches Seen\": {\"sum\": 1321.0, \"count\": 1, \"min\": 1321, \"max\": 1321}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:45 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=97451.43272694942 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:45 INFO 139757888653120] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=69.32089511828306\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:45 INFO 139757888653120] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=4805.3865\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:45 INFO 139757888653120] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=17.76055078125\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:38:47.572] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 2200, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:47 INFO 139757888653120] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=89.97940038085233\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:47 INFO 139757888653120] #quality_metric: host=algo-1, epoch=6, train mse <loss>=8096.292492897727\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:47 INFO 139757888653120] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=17.71759071377841\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462325.3680916, \"EndTime\": 1664462327.5731063, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2203.8798332214355, \"count\": 1, \"min\": 2203.8798332214355, \"max\": 2203.8798332214355}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:47 INFO 139757888653120] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462325.3692048, \"EndTime\": 1664462327.5732348, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1537633.0, \"count\": 1, \"min\": 1537633, \"max\": 1537633}, \"Total Batches Seen\": {\"sum\": 1541.0, \"count\": 1, \"min\": 1541, \"max\": 1541}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:47 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=99595.45652581433 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:47 INFO 139757888653120] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=69.21625892808713\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:47 INFO 139757888653120] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=4790.8905\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:47 INFO 139757888653120] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=17.675310546875\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:38:49.809] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 2232, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:49 INFO 139757888653120] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=89.8832945905348\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:49 INFO 139757888653120] #quality_metric: host=algo-1, epoch=7, train mse <loss>=8079.006646448864\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:49 INFO 139757888653120] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=17.618656507457388\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462327.5731537, \"EndTime\": 1664462329.8102, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2235.618829727173, \"count\": 1, \"min\": 2235.618829727173, \"max\": 2235.618829727173}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:49 INFO 139757888653120] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462327.5745525, \"EndTime\": 1664462329.8103733, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1757152.0, \"count\": 1, \"min\": 1757152, \"max\": 1757152}, \"Total Batches Seen\": {\"sum\": 1761.0, \"count\": 1, \"min\": 1761, \"max\": 1761}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:49 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=98177.6791563835 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:49 INFO 139757888653120] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=69.12159937385708\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:49 INFO 139757888653120] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=4777.7955\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:49 INFO 139757888653120] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=17.607453125\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:38:52.065] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 2250, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:52 INFO 139757888653120] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=89.79485394671846\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:52 INFO 139757888653120] #quality_metric: host=algo-1, epoch=8, train mse <loss>=8063.1157953125\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:52 INFO 139757888653120] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=17.545208456143467\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462329.8102474, \"EndTime\": 1664462332.0661833, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2254.7507286071777, \"count\": 1, \"min\": 2254.7507286071777, \"max\": 2254.7507286071777}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:52 INFO 139757888653120] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462329.8114111, \"EndTime\": 1664462332.0663197, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1976671.0, \"count\": 1, \"min\": 1976671, \"max\": 1976671}, \"Total Batches Seen\": {\"sum\": 1981.0, \"count\": 1, \"min\": 1981, \"max\": 1981}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:52 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=97348.24328099724 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:52 INFO 139757888653120] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=69.03410389075823\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:52 INFO 139757888653120] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=4765.7075\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:52 INFO 139757888653120] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=17.547384765625\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:38:54.320] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 2250, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:54 INFO 139757888653120] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=89.71333934953218\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:54 INFO 139757888653120] #quality_metric: host=algo-1, epoch=9, train mse <loss>=8048.483257244318\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:54 INFO 139757888653120] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=17.492578107244316\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462332.0662336, \"EndTime\": 1664462334.3209627, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2253.451108932495, \"count\": 1, \"min\": 2253.451108932495, \"max\": 2253.451108932495}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:54 INFO 139757888653120] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462332.0674863, \"EndTime\": 1664462334.3211727, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2196190.0, \"count\": 1, \"min\": 2196190, \"max\": 2196190}, \"Total Batches Seen\": {\"sum\": 2201.0, \"count\": 1, \"min\": 2201, \"max\": 2201}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:54 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=97400.41343086252 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:54 INFO 139757888653120] #quality_metric: host=algo-1, epoch=10, batch=0 train rmse <loss>=68.95205580691558\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:54 INFO 139757888653120] #quality_metric: host=algo-1, epoch=10, batch=0 train mse <loss>=4754.386\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:54 INFO 139757888653120] #quality_metric: host=algo-1, epoch=10, batch=0 train absolute_loss <loss>=17.48780859375\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:38:56.529] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 2203, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:56 INFO 139757888653120] #quality_metric: host=algo-1, epoch=10, train rmse <loss>=89.63795532316121\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:56 INFO 139757888653120] #quality_metric: host=algo-1, epoch=10, train mse <loss>=8034.963034517045\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:56 INFO 139757888653120] #quality_metric: host=algo-1, epoch=10, train absolute_loss <loss>=17.451685293856535\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462334.3210251, \"EndTime\": 1664462336.5295742, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2207.3683738708496, \"count\": 1, \"min\": 2207.3683738708496, \"max\": 2207.3683738708496}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:56 INFO 139757888653120] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462334.3221824, \"EndTime\": 1664462336.529784, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2415709.0, \"count\": 1, \"min\": 2415709, \"max\": 2415709}, \"Total Batches Seen\": {\"sum\": 2421.0, \"count\": 1, \"min\": 2421, \"max\": 2421}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:56 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=99433.09930810511 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:56 INFO 139757888653120] #quality_metric: host=algo-1, epoch=11, batch=0 train rmse <loss>=68.87502450090308\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:56 INFO 139757888653120] #quality_metric: host=algo-1, epoch=11, batch=0 train mse <loss>=4743.769\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:56 INFO 139757888653120] #quality_metric: host=algo-1, epoch=11, batch=0 train absolute_loss <loss>=17.43361328125\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:38:58.791] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 2258, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:58 INFO 139757888653120] #quality_metric: host=algo-1, epoch=11, train rmse <loss>=89.56831291800283\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:58 INFO 139757888653120] #quality_metric: host=algo-1, epoch=11, train mse <loss>=8022.482678977272\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:58 INFO 139757888653120] #quality_metric: host=algo-1, epoch=11, train absolute_loss <loss>=17.41848993696733\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462336.5296369, \"EndTime\": 1664462338.7916746, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2260.8420848846436, \"count\": 1, \"min\": 2260.8420848846436, \"max\": 2260.8420848846436}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:58 INFO 139757888653120] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462336.530809, \"EndTime\": 1664462338.791883, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2635228.0, \"count\": 1, \"min\": 2635228, \"max\": 2635228}, \"Total Batches Seen\": {\"sum\": 2641.0, \"count\": 1, \"min\": 2641, \"max\": 2641}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 13.0, \"count\": 1, \"min\": 13, \"max\": 13}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:58 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=97081.5131554058 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:58 INFO 139757888653120] #quality_metric: host=algo-1, epoch=12, batch=0 train rmse <loss>=68.8024272827638\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:58 INFO 139757888653120] #quality_metric: host=algo-1, epoch=12, batch=0 train mse <loss>=4733.774\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:38:58 INFO 139757888653120] #quality_metric: host=algo-1, epoch=12, batch=0 train absolute_loss <loss>=17.387841796875\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:39:01.014] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 2218, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:01 INFO 139757888653120] #quality_metric: host=algo-1, epoch=12, train rmse <loss>=89.50284904048382\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:01 INFO 139757888653120] #quality_metric: host=algo-1, epoch=12, train mse <loss>=8010.759986363636\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:01 INFO 139757888653120] #quality_metric: host=algo-1, epoch=12, train absolute_loss <loss>=17.386808993252842\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462338.7917378, \"EndTime\": 1664462341.0152118, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2222.163200378418, \"count\": 1, \"min\": 2222.163200378418, \"max\": 2222.163200378418}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:01 INFO 139757888653120] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462338.793027, \"EndTime\": 1664462341.0153558, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2854747.0, \"count\": 1, \"min\": 2854747, \"max\": 2854747}, \"Total Batches Seen\": {\"sum\": 2861.0, \"count\": 1, \"min\": 2861, \"max\": 2861}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:01 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=98775.25809546103 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:01 INFO 139757888653120] #quality_metric: host=algo-1, epoch=13, batch=0 train rmse <loss>=68.73475831047928\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:01 INFO 139757888653120] #quality_metric: host=algo-1, epoch=13, batch=0 train mse <loss>=4724.467\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:01 INFO 139757888653120] #quality_metric: host=algo-1, epoch=13, batch=0 train absolute_loss <loss>=17.341529296875\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:39:03.452] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 2433, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:03 INFO 139757888653120] #quality_metric: host=algo-1, epoch=13, train rmse <loss>=89.441471604372\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:03 INFO 139757888653120] #quality_metric: host=algo-1, epoch=13, train mse <loss>=7999.776842755682\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:03 INFO 139757888653120] #quality_metric: host=algo-1, epoch=13, train absolute_loss <loss>=17.36034843306108\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462341.015262, \"EndTime\": 1664462343.452759, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2436.171531677246, \"count\": 1, \"min\": 2436.171531677246, \"max\": 2436.171531677246}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:03 INFO 139757888653120] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462341.0165627, \"EndTime\": 1664462343.4529681, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3074266.0, \"count\": 1, \"min\": 3074266, \"max\": 3074266}, \"Total Batches Seen\": {\"sum\": 3081.0, \"count\": 1, \"min\": 3081, \"max\": 3081}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 15.0, \"count\": 1, \"min\": 15, \"max\": 15}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:03 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=90095.32261754804 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:03 INFO 139757888653120] #quality_metric: host=algo-1, epoch=14, batch=0 train rmse <loss>=68.67231611064243\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:03 INFO 139757888653120] #quality_metric: host=algo-1, epoch=14, batch=0 train mse <loss>=4715.887\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:03 INFO 139757888653120] #quality_metric: host=algo-1, epoch=14, batch=0 train absolute_loss <loss>=17.290421875\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:39:05.867] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 2409, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:05 INFO 139757888653120] #quality_metric: host=algo-1, epoch=14, train rmse <loss>=89.38374599610074\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:05 INFO 139757888653120] #quality_metric: host=algo-1, epoch=14, train mse <loss>=7989.454048295454\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:05 INFO 139757888653120] #quality_metric: host=algo-1, epoch=14, train absolute_loss <loss>=17.33161007191051\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462343.452822, \"EndTime\": 1664462345.8680956, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2413.9487743377686, \"count\": 1, \"min\": 2413.9487743377686, \"max\": 2413.9487743377686}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:05 INFO 139757888653120] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462343.4541256, \"EndTime\": 1664462345.8682306, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3293785.0, \"count\": 1, \"min\": 3293785, \"max\": 3293785}, \"Total Batches Seen\": {\"sum\": 3301.0, \"count\": 1, \"min\": 3301, \"max\": 3301}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:05 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=90928.88385383938 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:05 INFO 139757888653120] #quality_metric: host=algo-1, epoch=15, batch=0 train rmse <loss>=68.61336604481666\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:05 INFO 139757888653120] #quality_metric: host=algo-1, epoch=15, batch=0 train mse <loss>=4707.794\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:05 INFO 139757888653120] #quality_metric: host=algo-1, epoch=15, batch=0 train absolute_loss <loss>=17.246181640625\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:39:08.134] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 2263, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:08 INFO 139757888653120] #quality_metric: host=algo-1, epoch=15, train rmse <loss>=89.32937841500613\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:08 INFO 139757888653120] #quality_metric: host=algo-1, epoch=15, train mse <loss>=7979.737848011364\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:08 INFO 139757888653120] #quality_metric: host=algo-1, epoch=15, train absolute_loss <loss>=17.305939044744317\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462345.868147, \"EndTime\": 1664462348.1353633, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2265.927791595459, \"count\": 1, \"min\": 2265.927791595459, \"max\": 2265.927791595459}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:08 INFO 139757888653120] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462345.8694122, \"EndTime\": 1664462348.1355338, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3513304.0, \"count\": 1, \"min\": 3513304, \"max\": 3513304}, \"Total Batches Seen\": {\"sum\": 3521.0, \"count\": 1, \"min\": 3521, \"max\": 3521}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:08 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=96865.52868046475 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:08 INFO 139757888653120] #quality_metric: host=algo-1, epoch=16, batch=0 train rmse <loss>=68.55844222267598\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:08 INFO 139757888653120] #quality_metric: host=algo-1, epoch=16, batch=0 train mse <loss>=4700.26\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:08 INFO 139757888653120] #quality_metric: host=algo-1, epoch=16, batch=0 train absolute_loss <loss>=17.204728515625\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:39:10.355] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 2216, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:10 INFO 139757888653120] #quality_metric: host=algo-1, epoch=16, train rmse <loss>=89.27789061280922\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:10 INFO 139757888653120] #quality_metric: host=algo-1, epoch=16, train mse <loss>=7970.5417522727275\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:10 INFO 139757888653120] #quality_metric: host=algo-1, epoch=16, train absolute_loss <loss>=17.281078306995738\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462348.1354177, \"EndTime\": 1664462350.356328, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2219.607353210449, \"count\": 1, \"min\": 2219.607353210449, \"max\": 2219.607353210449}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:10 INFO 139757888653120] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462348.136701, \"EndTime\": 1664462350.3565118, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3732823.0, \"count\": 1, \"min\": 3732823, \"max\": 3732823}, \"Total Batches Seen\": {\"sum\": 3741.0, \"count\": 1, \"min\": 3741, \"max\": 3741}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:10 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=98886.85971016025 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:10 INFO 139757888653120] #quality_metric: host=algo-1, epoch=17, batch=0 train rmse <loss>=68.50691570929172\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:10 INFO 139757888653120] #quality_metric: host=algo-1, epoch=17, batch=0 train mse <loss>=4693.1975\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:10 INFO 139757888653120] #quality_metric: host=algo-1, epoch=17, batch=0 train absolute_loss <loss>=17.16900390625\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:39:12.553] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 2192, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:12 INFO 139757888653120] #quality_metric: host=algo-1, epoch=17, train rmse <loss>=89.22887229396252\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:12 INFO 139757888653120] #quality_metric: host=algo-1, epoch=17, train mse <loss>=7961.791650852273\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:12 INFO 139757888653120] #quality_metric: host=algo-1, epoch=17, train absolute_loss <loss>=17.25775461203835\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462350.3564007, \"EndTime\": 1664462352.5538657, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2196.2828636169434, \"count\": 1, \"min\": 2196.2828636169434, \"max\": 2196.2828636169434}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:12 INFO 139757888653120] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462350.3575585, \"EndTime\": 1664462352.55407, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3952342.0, \"count\": 1, \"min\": 3952342, \"max\": 3952342}, \"Total Batches Seen\": {\"sum\": 3961.0, \"count\": 1, \"min\": 3961, \"max\": 3961}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:12 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=99935.05240762299 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:12 INFO 139757888653120] #quality_metric: host=algo-1, epoch=18, batch=0 train rmse <loss>=68.45925065321705\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:12 INFO 139757888653120] #quality_metric: host=algo-1, epoch=18, batch=0 train mse <loss>=4686.669\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:12 INFO 139757888653120] #quality_metric: host=algo-1, epoch=18, batch=0 train absolute_loss <loss>=17.134076171875\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:39:14.757] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 2198, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:14 INFO 139757888653120] #quality_metric: host=algo-1, epoch=18, train rmse <loss>=89.182170988147\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:14 INFO 139757888653120] #quality_metric: host=algo-1, epoch=18, train mse <loss>=7953.459622159091\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:14 INFO 139757888653120] #quality_metric: host=algo-1, epoch=18, train absolute_loss <loss>=17.234091344105114\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462352.5539277, \"EndTime\": 1664462354.7576518, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2202.3210525512695, \"count\": 1, \"min\": 2202.3210525512695, \"max\": 2202.3210525512695}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:14 INFO 139757888653120] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462352.5553088, \"EndTime\": 1664462354.7577941, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4171861.0, \"count\": 1, \"min\": 4171861, \"max\": 4171861}, \"Total Batches Seen\": {\"sum\": 4181.0, \"count\": 1, \"min\": 4181, \"max\": 4181}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:14 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=99664.87387941488 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:14 INFO 139757888653120] #quality_metric: host=algo-1, epoch=19, batch=0 train rmse <loss>=68.41452331194013\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:14 INFO 139757888653120] #quality_metric: host=algo-1, epoch=19, batch=0 train mse <loss>=4680.547\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:14 INFO 139757888653120] #quality_metric: host=algo-1, epoch=19, batch=0 train absolute_loss <loss>=17.1045546875\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:39:16.965] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 2202, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:16 INFO 139757888653120] #quality_metric: host=algo-1, epoch=19, train rmse <loss>=89.13766491838494\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:16 INFO 139757888653120] #quality_metric: host=algo-1, epoch=19, train mse <loss>=7945.523307102273\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:16 INFO 139757888653120] #quality_metric: host=algo-1, epoch=19, train absolute_loss <loss>=17.212461692116477\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:16 INFO 139757888653120] #quality_metric: host=algo-1, train rmse <loss>=89.13766491838494\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:16 INFO 139757888653120] #quality_metric: host=algo-1, train mse <loss>=7945.523307102273\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:16 INFO 139757888653120] #quality_metric: host=algo-1, train absolute_loss <loss>=17.212461692116477\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462354.7577055, \"EndTime\": 1664462356.9660454, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2206.973075866699, \"count\": 1, \"min\": 2206.973075866699, \"max\": 2206.973075866699}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:16 INFO 139757888653120] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462354.7590487, \"EndTime\": 1664462356.9662507, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4391380.0, \"count\": 1, \"min\": 4391380, \"max\": 4391380}, \"Total Batches Seen\": {\"sum\": 4401.0, \"count\": 1, \"min\": 4401, \"max\": 4401}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:16 INFO 139757888653120] #throughput_metric: host=algo-1, train throughput=99450.34778707093 records/second\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:16 WARNING 139757888653120] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:16 INFO 139757888653120] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462356.9661148, \"EndTime\": 1664462356.9690862, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 2.5420188903808594, \"count\": 1, \"min\": 2.5420188903808594, \"max\": 2.5420188903808594}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:16 INFO 139757888653120] Saved checkpoint to \"/tmp/tmpk_ttf0_5/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:39:16.988] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 45548, \"num_examples\": 1, \"num_bytes\": 99820}\u001b[0m\n",
      "\u001b[34m[2022-09-29 14:39:17.207] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 219, \"num_examples\": 55, \"num_bytes\": 5462876}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462356.9883332, \"EndTime\": 1664462357.2079818, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 54880.0, \"count\": 1, \"min\": 54880, \"max\": 54880}, \"Total Batches Seen\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}, \"Max Records Seen Between Resets\": {\"sum\": 54880.0, \"count\": 1, \"min\": 54880, \"max\": 54880}, \"Max Batches Seen Between Resets\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 54880.0, \"count\": 1, \"min\": 54880, \"max\": 54880}, \"Number of Batches Since Last Reset\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}}}\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:17 INFO 139757888653120] #test_score (algo-1) : ('rmse', 68.73201116365345)\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:17 INFO 139757888653120] #test_score (algo-1) : ('mse', 4724.089358600583)\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:17 INFO 139757888653120] #test_score (algo-1) : ('absolute_loss', 17.038057946741755)\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:17 INFO 139757888653120] #quality_metric: host=algo-1, test rmse <loss>=68.73201116365345\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:17 INFO 139757888653120] #quality_metric: host=algo-1, test mse <loss>=4724.089358600583\u001b[0m\n",
      "\u001b[34m[09/29/2022 14:39:17 INFO 139757888653120] #quality_metric: host=algo-1, test absolute_loss <loss>=17.038057946741755\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664462356.9691494, \"EndTime\": 1664462357.2088299, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 23.44226837158203, \"count\": 1, \"min\": 23.44226837158203, \"max\": 23.44226837158203}, \"totaltime\": {\"sum\": 45804.81719970703, \"count\": 1, \"min\": 45804.81719970703, \"max\": 45804.81719970703}}}\u001b[0m\n",
      "\n",
      "2022-09-29 14:39:33 Uploading - Uploading generated training model\n",
      "2022-09-29 14:39:33 Completed - Training job completed\n",
      "Training seconds: 223\n",
      "Billable seconds: 223\n"
     ]
    }
   ],
   "source": [
    "if \"training_job_name\" not in locals():\n",
    "\n",
    "    fm.fit({\"train\": train_data_location, \"test\": test_data_location})\n",
    "    training_job_name = fm.latest_training_job.job_name\n",
    "\n",
    "else:\n",
    "    print(f\"Using previous training job: {training_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint. This will allow out to make predictions (or inference) from the model dyanamically.\n",
    "\n",
    "Note, Amazon SageMaker allows you the flexibility of importing models trained elsewhere, as well as the choice of not importing models if the target of model creation is AWS Lambda, AWS Greengrass, Amazon Redshift, Amazon Athena, or other deployment target.\n",
    "\n",
    "Here we will take the top customer, the customer who spent the most money, and try to find which items to recommend to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "class FMSerializer(JSONSerializer):\n",
    "    def serialize(self, data):\n",
    "        js = {\"instances\": []}\n",
    "        for row in data:\n",
    "            js[\"instances\"].append({\"features\": row.tolist()})\n",
    "        return json.dumps(js)\n",
    "\n",
    "\n",
    "fm_predictor = fm.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    serializer=FMSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find customer who spent the most money\n",
    "df = pd.read_csv(\"data/online_retail_preprocessed.csv\")\n",
    "\n",
    "df[\"invoice_amount\"] = df[\"Quantity\"] * df[\"UnitPrice\"]\n",
    "top_customer = (\n",
    "    df.groupby(\"CustomerID\").sum()[\"invoice_amount\"].sort_values(ascending=False).index[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(df, customer_id, n_recommendations, n_ranks=100):\n",
    "    popular_items = (\n",
    "        df.groupby([\"StockCode\", \"UnitPrice\"])\n",
    "        .nunique()[\"CustomerID\"]\n",
    "        .sort_values(ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    top_n_items = popular_items[\"StockCode\"].iloc[:n_ranks].values\n",
    "    top_n_prices = popular_items[\"UnitPrice\"].iloc[:n_ranks].values\n",
    "\n",
    "    # stock codes can have multiple descriptions, so we will choose whichever description is most common\n",
    "    item_map = df.groupby(\"StockCode\").agg(lambda x: x.value_counts().index[0])[\"Description\"]\n",
    "\n",
    "    # find customer's country\n",
    "    df_subset = df.loc[df[\"CustomerID\"] == customer_id]\n",
    "    country = df_subset[\"Country\"].value_counts().index[0]\n",
    "\n",
    "    data = []\n",
    "    flattened_item_map = [item_map[i] for i in top_n_items]\n",
    "    for idx in range(len(top_n_items)):\n",
    "        data.append(\n",
    "            {\n",
    "                \"StockCode\": top_n_items[idx],\n",
    "                \"Description\": flattened_item_map[idx],\n",
    "                \"CustomerID\": customer_id,\n",
    "                \"Country\": country,\n",
    "                \"UnitPrice\": top_n_prices[idx],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_inference = pd.DataFrame(data)\n",
    "\n",
    "    # we need to build the data set similar to how we built it for training\n",
    "    # it should have the same number of features as the training data\n",
    "    enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    onehot_cols = [\"StockCode\", \"CustomerID\", \"Country\"]\n",
    "    enc.fit(df[onehot_cols])\n",
    "    onehot_output = enc.transform(df_inference[onehot_cols])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=2)\n",
    "    unique_descriptions = df[\"Description\"].unique()\n",
    "    vectorizer.fit(unique_descriptions)\n",
    "    tfidf_output = vectorizer.transform(df_inference[\"Description\"])\n",
    "\n",
    "    row = range(len(df_inference))\n",
    "    col = [0] * len(df_inference)\n",
    "    unit_price = csr_matrix((df_inference[\"UnitPrice\"].values, (row, col)), dtype=\"float32\")\n",
    "\n",
    "    X_inference = hstack([onehot_output, tfidf_output, unit_price], format=\"csr\")\n",
    "\n",
    "    result = fm_predictor.predict(X_inference.toarray())\n",
    "    preds = [i[\"score\"] for i in result[\"predictions\"]]\n",
    "    index_array = np.array(preds).argsort()\n",
    "    items = enc.inverse_transform(onehot_output)[:, 0]\n",
    "    top_recs = np.take_along_axis(items, index_array, axis=0)[: -n_recommendations - 1 : -1]\n",
    "    recommendations = [[i, item_map[i]] for i in top_recs]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended products:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['22423', 'REGENCY CAKESTAND 3 TIER'],\n",
       " ['22776', 'SWEETHEART CAKESTAND 3 TIER'],\n",
       " ['22624', 'IVORY KITCHEN SCALES'],\n",
       " ['85123A', 'WHITE HANGING HEART T-LIGHT HOLDER'],\n",
       " ['85099B', 'JUMBO BAG RED RETROSPOT']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 5 recommended products:\")\n",
    "get_recommendations(df, top_customer, n_recommendations=5, n_ranks=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done with the endpoint, you should delete the endpoint to save cost and free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.delete_model()\n",
    "fm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Part: Registering the Model in SageMaker Model Registry\n",
    "\n",
    "Once a useful model has been trained, you have the option to register the model for future reference and possible deployment. To do so, we must first properly associate the artifacts of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_info = sagemaker_boto_client.describe_training_job(TrainingJobName=training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing artifact: arn:aws:sagemaker:ap-northeast-2:337402169710:artifact/0ceb136d4dc1412728332fd274d0dcc9\n"
     ]
    }
   ],
   "source": [
    "training_data_s3_uri = training_job_info[\"InputDataConfig\"][0][\"DataSource\"][\"S3DataSource\"][\n",
    "    \"S3Uri\"\n",
    "]\n",
    "\n",
    "matching_artifacts = list(\n",
    "    artifact.Artifact.list(source_uri=training_data_s3_uri, sagemaker_session=sagemaker_session)\n",
    ")\n",
    "\n",
    "if matching_artifacts:\n",
    "    training_data_artifact = matching_artifacts[0]\n",
    "    print(f\"Using existing artifact: {training_data_artifact.artifact_arn}\")\n",
    "else:\n",
    "    training_data_artifact = artifact.Artifact.create(\n",
    "        artifact_name=\"TrainingData\",\n",
    "        source_uri=training_data_s3_uri,\n",
    "        artifact_type=\"Dataset\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    print(f\"Create artifact {training_data_artifact.artifact_arn}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Artifact\n",
    "\n",
    "We do not need a code artifact because we are using a built-in SageMaker Algorithm called Factorization Machines. The Factorization Machines container contains all of the code and, by default, our model training stores the Factorization Machines image for tracking purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing artifact: arn:aws:sagemaker:ap-northeast-2:337402169710:artifact/a70e3a49bba907c2fd35e42b1cb4fa81\n"
     ]
    }
   ],
   "source": [
    "trained_model_s3_uri = training_job_info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "matching_artifacts = list(\n",
    "    artifact.Artifact.list(source_uri=trained_model_s3_uri, sagemaker_session=sagemaker_session)\n",
    ")\n",
    "\n",
    "if matching_artifacts:\n",
    "    model_artifact = matching_artifacts[0]\n",
    "    print(f\"Using existing artifact: {model_artifact.artifact_arn}\")\n",
    "else:\n",
    "    model_artifact = artifact.Artifact.create(\n",
    "        artifact_name=\"TrainedModel\",\n",
    "        source_uri=trained_model_s3_uri,\n",
    "        artifact_type=\"Model\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    print(f\"Create artifact {model_artifact.artifact_arn}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set artifact associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_component = sagemaker_boto_client.describe_trial_component(\n",
    "    TrialComponentName=training_job_name + \"-aws-training-job\"\n",
    ")\n",
    "trial_component_arn = trial_component[\"TrialComponentArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association already exists with DataSet\n",
      "Association with Model: SUCCEESFUL\n"
     ]
    }
   ],
   "source": [
    "artifact_list = [[training_data_artifact, \"ContributedTo\"], [model_artifact, \"Produced\"]]\n",
    "\n",
    "for art, assoc in artifact_list:\n",
    "    try:\n",
    "        association.Association.create(\n",
    "            source_arn=art.artifact_arn,\n",
    "            destination_arn=trial_component_arn,\n",
    "            association_type=assoc,\n",
    "            sagemaker_session=sagemaker_session,\n",
    "        )\n",
    "        print(f\"Association with {art.artifact_type}: SUCCEESFUL\")\n",
    "    except:\n",
    "        print(f\"Association already exists with {art.artifact_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model retail-recommendations\n"
     ]
    }
   ],
   "source": [
    "model_name = \"retail-recommendations\"\n",
    "model_matches = sagemaker_boto_client.list_models(NameContains=model_name)[\"Models\"]\n",
    "\n",
    "if not model_matches:\n",
    "    print(f\"Creating model {model_name}\")\n",
    "    model = sagemaker_session.create_model_from_job(\n",
    "        name=model_name,\n",
    "        training_job_name=training_job_info[\"TrainingJobName\"],\n",
    "        role=sagemaker_role,\n",
    "        image_uri=training_job_info[\"AlgorithmSpecification\"][\"TrainingImage\"],\n",
    "    )\n",
    "else:\n",
    "    print(f\"Model {model_name} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model Package Group\n",
    "\n",
    "After associating all the relevant artifacts, the Model Package Group can now be created. A Model Package Groups holds multiple versions or iterations of a model. Though it is not required to create them for every model in the registry, they help organize various models which all have the same purpose and provide autiomatic versioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Package Group name: retail-recommendation-2022-09-29-14-44\n"
     ]
    }
   ],
   "source": [
    "if \"mpg_name\" not in locals():\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    mpg_name = f\"retail-recommendation-{timestamp}\"\n",
    "\n",
    "print(f\"Model Package Group name: {mpg_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_input_dict = {\n",
    "    \"ModelPackageGroupName\": mpg_name,\n",
    "    \"ModelPackageGroupDescription\": \"Recommendation for Online Retail Sales\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Model Package Group retail-recommendation-2022-09-29-14-44: SUCCESSFUL\n"
     ]
    }
   ],
   "source": [
    "matching_mpg = sagemaker_boto_client.list_model_package_groups(NameContains=mpg_name)[\n",
    "    \"ModelPackageGroupSummaryList\"\n",
    "]\n",
    "\n",
    "if matching_mpg:\n",
    "    print(f\"Using existing Model Package Group: {mpg_name}\")\n",
    "else:\n",
    "    mpg_response = sagemaker_boto_client.create_model_package_group(**mpg_input_dict)\n",
    "    print(f\"Create Model Package Group {mpg_name}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics_report = {\"regression_metrics\": {}}\n",
    "\n",
    "for metric in training_job_info[\"FinalMetricDataList\"]:\n",
    "    stat = {metric[\"MetricName\"]: {\"value\": metric[\"Value\"]}}\n",
    "    model_metrics_report[\"regression_metrics\"].update(stat)\n",
    "\n",
    "with open(\"training_metrics.json\", \"w\") as f:\n",
    "    json.dump(model_metrics_report, f)\n",
    "\n",
    "metrics_s3_key = f\"training_jobs/{training_job_info['TrainingJobName']}/training_metrics.json\"\n",
    "s3_client.upload_file(Filename=\"training_metrics.json\", Bucket=bucket, Key=metrics_s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the inference spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_inference_spec = InferenceSpecification().get_inference_specification_dict(\n",
    "    ecr_image=training_job_info[\"AlgorithmSpecification\"][\"TrainingImage\"],\n",
    "    supports_gpu=False,\n",
    "    supported_content_types=[\"application/x-recordio-protobuf\", \"application/json\"],\n",
    "    supported_mime_types=[\"text/csv\"],\n",
    ")\n",
    "\n",
    "mp_inference_spec[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"] = training_job_info[\n",
    "    \"ModelArtifacts\"\n",
    "][\"S3ModelArtifacts\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model metrics\n",
    "\n",
    "Metrics other than model quality can be defined. See the Boto3 documentation for [creating a model package](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model_package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = {\n",
    "    \"ModelQuality\": {\n",
    "        \"Statistics\": {\n",
    "            \"ContentType\": \"application/json\",\n",
    "            \"S3Uri\": f\"s3://{bucket}/{metrics_s3_key}\",\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_input_dict = {\n",
    "    \"ModelPackageGroupName\": mpg_name,\n",
    "    \"ModelPackageDescription\": \"Factorization Machine Model to create personalized retail recommendations\",\n",
    "    \"ModelApprovalStatus\": \"PendingManualApproval\",\n",
    "    \"ModelMetrics\": model_metrics,\n",
    "}\n",
    "\n",
    "mp_input_dict.update(mp_inference_spec)\n",
    "mp_response = sagemaker_boto_client.create_model_package(**mp_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait until model package is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model package status: Completed\n"
     ]
    }
   ],
   "source": [
    "mp_info = sagemaker_boto_client.describe_model_package(\n",
    "    ModelPackageName=mp_response[\"ModelPackageArn\"]\n",
    ")\n",
    "mp_status = mp_info[\"ModelPackageStatus\"]\n",
    "\n",
    "while mp_status not in [\"Completed\", \"Failed\"]:\n",
    "    time.sleep(5)\n",
    "    mp_info = sagemaker_boto_client.describe_model_package(\n",
    "        ModelPackageName=mp_response[\"ModelPackageArn\"]\n",
    "    )\n",
    "    mp_status = mp_info[\"ModelPackageStatus\"]\n",
    "    print(f\"model package status: {mp_status}\")\n",
    "print(f\"model package status: {mp_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package = sagemaker_boto_client.list_model_packages(ModelPackageGroupName=mpg_name)[\n",
    "    \"ModelPackageSummaryList\"\n",
    "][0]\n",
    "model_package_update = {\n",
    "    \"ModelPackageArn\": model_package[\"ModelPackageArn\"],\n",
    "    \"ModelApprovalStatus\": \"Approved\",\n",
    "}\n",
    "\n",
    "update_response = sagemaker_boto_client.update_model_package(**model_package_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...2-09-29-14-34-33-631/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...69710/personalization/test/test.protobuf</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...710/personalization/train/train.protobuf</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83516...2.amazonaws.com/factorization-machines:1</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...2-09-29-14-34-33-631/output/model.tar.gz</td>\n",
       "      <td>Output</td>\n",
       "      <td>Model</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...2-09-29-14-34-33-631/output/model.tar.gz     Input    Model   \n",
       "1  s3://...69710/personalization/test/test.protobuf     Input  DataSet   \n",
       "2  s3://...710/personalization/train/train.protobuf     Input  DataSet   \n",
       "3  83516...2.amazonaws.com/factorization-machines:1     Input    Image   \n",
       "4  s3://...2-09-29-14-34-33-631/output/model.tar.gz    Output    Model   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0         Produced     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3    ContributedTo     artifact  \n",
       "4         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker_session)\n",
    "display(viz.show(training_job_name=training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-2:806072073708:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
